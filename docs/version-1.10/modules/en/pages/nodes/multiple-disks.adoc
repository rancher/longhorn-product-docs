= Multiple Disks
:description: Configure additional disks for Longhorn storage by adding a new disk, mounting it to a directory on the host, and then using either the UI or kubectl.
:revdate: 2025-07-04
:page-revdate: {revdate}
:current-version: {page-component-version}

{longhorn-product-name} supports using more than one disk on the nodes to store the volume data.

By default, {longhorn-product-name} stores volume data in the `/var/lib/longhorn` directory on the host. To use a different disk for storage, you can add a new disk and disable scheduling for the default directory. This approach provides the flexibility to manage storage based on your specific requirements.

== Add a Disk

Before adding a disk to {longhorn-product-name}, mount it to a directory on the Longhorn node's host.

. **Choose a Disk**: Select the physical or virtual disk for Longhorn storage and format it with an extent-based filesystem (for example, ext4 or XFS).
. **Mount the Disk**: Mount the disk to a directory on the host, such as `/mnt/example-disk`. Ensure the directory is accessible and correctly configured.

After the disk is mounted, you can add it to {longhorn-product-name} by using either the UI or the `kubectl` command-line tool.

*Using the {longhorn-product-name} UI*

. Go to the *Nodes* tab, select a node, and choose *Edit Disks* from the dropdown menu.
. Add the disk's mount path to the disk list.

*Using `kubectl`*

. Run `kubectl edit node.longhorn.io <node-name>` to modify the Longhorn node resource.
. Add the disk path to `spec.disks`. For example:
+
[,yaml]
----
...
spec:
  ...
  disks:
    ...
    example-disk:
      allowScheduling: true
      diskDriver: ""
      diskType: filesystem
      evictionRequested: false
      path: /mnt/example-disk
      storageReserved: 0
      tags: []
...
----
+
. Save and exit the editor.

Once a disk is added:

* {longhorn-product-name} automatically detects the disk's storage details, such as maximum and available capacity.
* If the disk is suitable for storing volume data, {longhorn-product-name} begins scheduling volumes to it.

[NOTE]
====
. You cannot add a disk path that another {longhorn-product-name} disk already uses.
. {longhorn-product-name} uses the filesystem ID to detect duplicate mounts. Therefore, you cannot add a disk with the *same filesystem ID* as another disk on the same node.
+
For more information, see issue https://github.com/longhorn/longhorn/issues/2477[#2477].
====

=== Root Disk Reservation

Optionally, you can use the *Space Reserved* field in the UI or `spec.disks.<disk-name>.storageReserved` to reserve a portion of disk space (in bytes) for other purposes. {longhorn-product-name} will not use this reserved space for volume data.

To maintain node stability when compute resources (for example, memory or disk) are under pressure, `kubelet` requires some space to remain free. If these critical resources are exhausted, it can lead to node instability.

By default, {longhorn-product-name} reserves 30% of the root disk space (`/var/lib/longhorn`) to prevent issues such as `DiskPressure` conditions from `kubelet`, especially after scheduling multiple volumes. The `storage-reserved-percentage-for-default-disk` setting controls this behavior.

=== Use an Alternative Path for a Disk on the Node

If you prefer to use a different path for a disk (rather than the original mount point), you can use `mount --bind` to create an alternative path. Do *not* use a symbolic link (`ln -s`), as symbolic links are not properly resolved inside Longhorn pods.

Ensure the alternative path is remounted after a node reboot, for example, by adding it to `/etc/fstab`.

== Remove a Disk

Nodes and disks can be excluded from future scheduling. Note that any storage already scheduled on a node will not be automatically released when scheduling is disabled for that node.

To remove a disk:

. Disable scheduling for the disk.
. Ensure that no replicas or backing images remain on the disk, including any in an error state. For instructions on how to evict replicas from disabled disks, see xref:nodes/disks-or-nodes-eviction.adoc#_select_disks_or_nodes_for_eviction[Select Disks or Nodes for Eviction].

Once the disk is empty and scheduling is disabled, you can safely remove it from the node configuration.

== Configuration

Two global settings affect volume scheduling:

* `StorageOverProvisioningPercentage` defines the maximum total storage that can be *scheduled* on a disk, relative to its usable capacity. The formula is:

[,text]
----
ScheduledStorage / (MaximumStorage - ReservedStorage)
----

By default, this setting is `100`%.

For example, on a 200 GiB disk with 50 GiB reserved, {longhorn-product-name} considers 150 GiB of usable space. With the default setting, {longhorn-product-name} can schedule up to 150 GiB of volume data.

Because workloads typically do not consume the entire allocated volume size, and {longhorn-product-name} uses sparse files to store data, increasing this setting is generally safe and can help optimize disk utilization.

* `StorageMinimalAvailablePercentage` specifies the minimum percentage of free space that must remain on a disk to schedule new replicas. The formula is:

[,text]
----
AvailableStorage / MaximumStorage
----

By default, this setting is `25`%.

For example, for a 200 GiB disk with 50 GiB reserved, {longhorn-product-name} stops scheduling new replicas if available space falls below 37.5 GiB (25% of 150 GiB). A new volume also will not be scheduled if its size would push available space below that limit.

This setting helps prevent disks from becoming too full, which could lead to scheduling failures or volume operation issues.

[WARNING]
====
{longhorn-product-name} currently cannot fully enforce the `StorageMinimalAvailablePercentage` limit in all scenarios because:

. Longhorn volumes might use more space than their requested size, especially when snapshots are taken.
. {longhorn-product-name} allows over-provisioning by default.
====
